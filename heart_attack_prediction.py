# -*- coding: utf-8 -*-
"""Heart attack prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AYyH_gaY1vTGnVi2AGec9lc7Jir2jieZ

# Heart Attack - Data Analysis and Prediction modelling

Author: Y. Staeva

This project is focused on Heart attack data analysis and prediction modelling.

The data used for the project is available at:


R. Rahman (2021): Heart Attack Analysis and Prediction

Dataset link: https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/version/2?select=heart.csv


The main purpose of the following code is to perform exploratory data analysis on the given dataset and to construct two classification models in order to compare their performance.

### Exploratory Data Analysis

**Install Pandas profiling**
"""

! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

"""**Import Numpy, Pandas and Pandas profiling**"""

import numpy as np
import pandas as pd
from pandas_profiling import ProfileReport

"""**Read the dataset**"""

from google.colab import drive
drive.mount('/content/drive')

heart_data = pd.read_csv('/content/drive/MyDrive/heart.csv')

"""**Check if the dataset is correctly read**"""

heart_data.head()

heart_data.columns

"""**Generate a report on Exploratory data analysis using Pandas profiling**"""

report = ProfileReport(heart_data)

report

"""### Predictive variables

In this section the predictive variables will be selected and classified into coarse classes. The predictive variables must not have a correlation value with the target (outcome) variables, that is greater or lesses than 0.5. Classification of numerical variables, such as age, should indicate either positive or negative relationship between the variable class and the outcome.

**The first predictive varibale is the sex of the patient.**
"""

heart_data['sex'].value_counts(dropna=False)

heart_data.groupby(['sex'])['output'].mean()

"""**Cholestoral concentration of the patient in mg/dl.**"""

heart_data['chol'].describe()

heart_data['Cholesterol_conc'] = pd.cut(heart_data['chol'],[126, 211, 240, 274, 600], right=False)

heart_data['Cholesterol_conc'].value_counts(dropna=False).sort_index()

heart_data.groupby(['Cholesterol_conc'])['output'].mean()

"""**Resting electrocardiographic results**

Value 0 - normal

Value 1 - ST-T wave abnormality

Value 2 - probable left ventricular hypertrophy
"""

heart_data['restecg'].value_counts(dropna=False)

heart_data['Rest_ecg_class'] = np.where(heart_data['restecg'] == 0, 'Normal', 'At risk')

heart_data['Rest_ecg_class'].value_counts(dropna=False).sort_index()

heart_data.groupby(['Rest_ecg_class'])['output'].mean()

"""**Chest pain**

Value 1: Typical angina

Value 2: Atypical angina

Value 3: Non - anginal pain

Value 4: Asymptomatic
"""

heart_data['cp'].value_counts(dropna=False)

heart_data.groupby(['cp'])['output'].mean()

heart_data['Chest_pain_class'] = np.where(heart_data['cp'] == 0, 'Lower risk', 'Higher risk')

heart_data['Chest_pain_class'].value_counts(dropna=False).sort_index()

heart_data.groupby(['Chest_pain_class'])['output'].mean()

"""**Maximum heart rate achieved**"""

heart_data['thalachh'].describe()

heart_data['Max_heart_rate_class'] = pd.cut(heart_data['thalachh'], [60, 133, 153, 166, 210], right=False)

heart_data['Max_heart_rate_class'].value_counts(dropna=False)

heart_data.groupby(['Max_heart_rate_class'])['output'].mean()

"""**Resting blood sugar pressure**"""

heart_data['trtbps'].describe()

heart_data['Resting_blood_pressure_class'] = pd.cut(heart_data['trtbps'], [90, 120, 140, 220], right=False)

heart_data['Resting_blood_pressure_class'].value_counts(dropna=False).sort_index()

heart_data.groupby(['Resting_blood_pressure_class'])['output'].mean()

"""**Fasting blood sugar**

Value 1: fbs > 120 mg/dl

Value 0: fbs < 120 mg/dl
"""

heart_data['fbs'].value_counts(dropna=False)

heart_data.groupby(['fbs'])['output'].mean()

"""### Data processing

**The selected predictive variables are:**

Sex, cholestoral concentration, resting electrocardiographic results, chest pain, maximum heart rate achieved, resting blood sugar pressure and fasting blood sugar.

First, the predictive variables, as well as their classing, are copied in a new dataframe called heart_model.
"""

heart_data.columns

heart_model = heart_data[['output', 'sex', 'chol', 'Cholesterol_conc', 'restecg', 'Rest_ecg_class', 'cp', 'Chest_pain_class', 'thalachh', 'Max_heart_rate_class', 'trtbps', 'Resting_blood_pressure_class', 'fbs']].copy()

heart_model.columns

"""In the next step, the classified variables are represented with the use of dummy variables."""

heart_model_dummy = pd.concat([heart_model['output'],
                         pd.get_dummies(heart_model['sex'], prefix='sex', dummy_na=False),
                         pd.get_dummies(heart_model['Cholesterol_conc'], prefix='chol', dummy_na=False),
                         pd.get_dummies(heart_model['Rest_ecg_class'], prefix='restecg', dummy_na=False),
                         pd.get_dummies(heart_model['Chest_pain_class'], prefix='cp', dummy_na=False),
                         pd.get_dummies(heart_model['Max_heart_rate_class'], prefix='thalachh', dummy_na=False),
                         pd.get_dummies(heart_model['Resting_blood_pressure_class'], prefix='trtbps', dummy_na=False),
                         pd.get_dummies(heart_model['fbs'], prefix='fbs', dummy_na=False)], axis = 1, ignore_index=False, join = 'outer')

heart_model_dummy.head()

heart_model_dummy.shape

"""The dataframe holding the predictive variables is converted into a numpy vector in order to be divided into features and target vectors."""

from sklearn.model_selection import train_test_split

heart_model_vector = heart_model_dummy.to_numpy()

heart_model_vector.shape

features = heart_model_vector[:, 1 : 20]

features.shape

features

target = heart_model_vector[:, 0]

target.shape

target

x_train, x_test, y_train, y_test = train_test_split(features, target, train_size = 0.7, random_state=42)

x_train.shape

y_train.shape

"""### Model development

In this step I will create and evaluate the performance of two classification models. The first method is Logistic Regression and the second method is a Support Vector Machine.The evaluation is done with a Confusion matrix, as well as calculating the Sensitivity and Specificity for both methods.
"""

import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

"""**Create and plot a Confusion Matrix**"""

def plot_confusion_matrix(confusion_matrix, classes, normalize = False, title = 'Confusion Matrix', cmap = plt.cm.Blues):
  plt.imshow(confusion_matrix, interpolation = 'nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation = 45)
  plt.yticks(tick_marks,classes)

  print(confusion_matrix)

"""**Logistic Regression**"""

logistic_regression = LogisticRegression(random_state=0, solver="sag")

lg_model = logistic_regression.fit(x_train,y_train)

lg_prediction = lg_model.predict(x_test)

lg_rounded_prediction = np.argmax(lg_prediction, axis =-1)

lg_confusion_matrix = confusion_matrix(y_true = y_test, y_pred = lg_prediction)

lg_classes = ['Higher Risk patient', 'Lower Risk patient']

plot_confusion_matrix(lg_confusion_matrix, classes = lg_classes)

"""Logistic Regression metrics:

Sensitivity: 0.7805

Specificity: 0.76

**Support Vector Machine**
"""

from sklearn.svm import LinearSVC

support_vector_classifier = LinearSVC(C = 1.0)

svc_model = support_vector_classifier.fit(x_train,y_train)

svc_predictions = svc_model.predict(x_test)

svc_rounded_predictions = np.argmax(svc_predictions)

svc_confusion_matrix = confusion_matrix(y_true = y_test, y_pred = svc_predictions)

svc_classes = ['Higher Risk patient', 'Lower Risk patient']

plot_confusion_matrix(svc_confusion_matrix, classes = svc_classes)

"""Support Vector Machine metrics:

Sensitivity: 0.7561

Specificity: 0.78

**Conclusion:**

The results show, that after calculatiting the Sensitivity and Specificity for each model, both methods tend to have a good performance. In order to select a "better" solution for the problem, there are two questions, that arise:

Q1: Is it more important to detect patients with Higher Heart Attack Risk?

If so, then it would be appropriate to select the Logistic regression model, as it has a higher Sensitivity value.

Q2: Is it more important to detect patients with Lower Heart Attack Risk?

If so, then the Support Vector Machine method is more suitable for the problem, as it has a higher Specificity value.
"""